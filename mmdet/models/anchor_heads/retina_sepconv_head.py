import numpy as np
import math
import torch
import torch.nn as nn
from mmcv.cnn import (normal_init, xavier_init, kaiming_init)
# from mmcv.cnn import (normal_init, xavier_init, kaiming_init,
#                       xavier_trunc_normal_init, kaiming_trunc_normal_init)

# from ..registry import HEADS
# from ..utils import ConvModule, bias_init_with_prob
# from .anchor_head import AnchorHead

from mmdet.ops import ConvModule
from mmdet.core import multi_apply
from ..registry import HEADS
from ..utils import bias_init_with_prob
from .anchor_head import AnchorHead
from mmdet.ops import build_norm_layer

from mmdet.core import (AnchorGenerator, anchor_target, delta2bbox, force_fp32,
                        multi_apply, multiclass_nms)


# 46799: add truncated normal similar to tensorflow, copy from pytorch-master branch


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def xavier_trunc_normal_(tensor, gain=1.):
    fan_in, fan_out = nn.init._calculate_fan_in_and_fan_out(tensor)
    # constant taken from scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)
    std = gain * math.sqrt(2.0 / float(fan_in + fan_out)) / .87962566103423978
    a = -2. * std
    b = 2. * std
    return _no_grad_trunc_normal_(tensor, mean=0., std=std, a=a, b=b)


def kaiming_trunc_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu'):
    fan = nn.init._calculate_correct_fan(tensor, mode)
    gain = nn.init.calculate_gain(nonlinearity, a)
    std = gain / math.sqrt(fan) / .87962566103423978
    a = -2. * std
    b = 2. * std
    with torch.no_grad():
        return _no_grad_trunc_normal_(tensor, mean=0., std=std, a=a, b=b)
#####################################################################################################################
# 46799: add truncated normal initialization


def xavier_trunc_normal_init(moudle, gain=1., bias=0):
    xavier_trunc_normal_(module.weight, gain=gain)
    if hasattr(module, 'bias') and module.bias is not None:
        nn.init.constant_(module.bias, bias)


def trunc_normal_init(module, mean=0, std=1., bias=0.):
    _no_grad_trunc_normal_(module.weight, mean, std)
    if hasattr(module, 'bias') and module.bias is not None:
        nn.init.constant_(module.bias, bias)


def kaiming_trunc_normal_init(module,
                              a=0,
                              mode='fan_out',
                              nonlinearity='relu',
                              bias=0):
    kaiming_trunc_normal_(module.weight, a=a, mode=mode,
                          nonlinearity=nonlinearity)
    if hasattr(module, 'bias') and module.bias is not None:
        nn.init.constant_(module.bias, bias)
#########################################################################################################

class Swish(nn.Module):
    def forward(self, x):
        return x * torch.sigmoid(x)


class SeparableConv(nn.Module):

    def __init__(self,
                 in_channels,
                 out_channels,
                 bias=False,
                 relu=False):
        super(SeparableConv, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.relu = relu

        self.sep = nn.Conv2d(in_channels,
                             in_channels,
                             3,
                             padding=1,
                             groups=in_channels,
                             bias=False)
        self.pw = nn.Conv2d(
            in_channels,
            out_channels,
            1,
            bias=bias)
        if relu:
            self.relu_fn = Swish()

    def forward(self, x):
        x = self.pw(self.sep(x))
        if self.relu:
            x = self.relu_fn(x)
        return x


@HEADS.register_module
class RetinaSepConvHead(AnchorHead):
    """
    retina head with separable convolution
    """

    def __init__(self,
                 num_classes,
                 in_channels,
                 stacked_convs=4,
                 octave_base_scale=4,
                 scales_per_octave=3,
                 conv_cfg=None,
                 norm_cfg=dict(type='SyncBN', momentum=0.01,
                            eps=1e-3, requires_grad=True),
                 **kwargs):
        self.stacked_convs = stacked_convs
        self.octave_base_scale = octave_base_scale
        self.scales_per_octave = scales_per_octave
        self.conv_cfg = conv_cfg
        self.norm_cfg = norm_cfg
        octave_scales = np.array(
            [2**(i / scales_per_octave) for i in range(scales_per_octave)])
        anchor_scales = octave_scales * octave_base_scale
        super(RetinaSepConvHead, self).__init__(
            num_classes, in_channels, anchor_scales=anchor_scales, **kwargs)

    def _init_layers(self):
        self.levels = len(self.anchor_strides)
        self.relu = nn.ReLU(inplace=True)
        self.cls_convs = nn.ModuleList()
        self.reg_convs = nn.ModuleList()
        self.cls_bn_levels = nn.ModuleList()
        self.reg_bn_levels = nn.ModuleList()
        for i in range(self.stacked_convs):
            chn = self.in_channels if i == 0 else self.feat_channels
            self.cls_convs.append(SeparableConv(chn,
                                                self.feat_channels,
                                                bias=True,
                                                relu=False))
            cls_bn_level = nn.ModuleList()
            for l in range(self.levels):
                _, bn_layer = build_norm_layer(self.norm_cfg, self.feat_channels)
                cls_bn_level.append(
                    nn.Sequential(
                        bn_layer,
                        Swish()
                    ))
            self.cls_bn_levels.append(cls_bn_level)
            self.reg_convs.append(SeparableConv(chn,
                                                self.feat_channels,
                                                bias=True,
                                                relu=False))
            reg_bn_level = nn.ModuleList()
            for l in range(self.levels):
                _, bn_layer = build_norm_layer(self.norm_cfg, self.feat_channels)
                reg_bn_level.append(
                    nn.Sequential(
                        bn_layer,
                        Swish()))
            self.reg_bn_levels.append(reg_bn_level)
        self.retina_cls = SeparableConv(
            self.feat_channels,
            self.num_anchors*self.cls_out_channels,
            bias=True,
            relu=False)
        self.retina_reg = SeparableConv(
            self.feat_channels,
            self.num_anchors * 4,
            bias=True,
            relu=False)

    def init_weights(self):
        for m in self.cls_convs:
            for mod in m.modules():
                if isinstance(mod, nn.Conv2d):
                    kaiming_trunc_normal_init(
                        mod, mode='fan_out', nonlinearity='sigmoid')
                    # kaiming_init(mod, mode='fan_in')
        for m in self.reg_convs:
            for mod in m.modules():
                if isinstance(mod, nn.Conv2d):
                    kaiming_trunc_normal_init(
                        mod, mode='fan_out', nonlinearity='sigmoid')
                    # kaiming_init(mod, mode='fan_in')
        bias_cls = bias_init_with_prob(0.01)
        kaiming_trunc_normal_init(
            self.retina_cls.sep, mode='fan_out', nonlinearity='sigmoid')
        kaiming_trunc_normal_init(
            self.retina_cls.pw, mode='fan_out', nonlinearity='sigmoid', bias=bias_cls)
        kaiming_trunc_normal_init(
            self.retina_reg.sep, mode='fan_out',  nonlinearity='sigmoid')
        kaiming_trunc_normal_init(
            self.retina_reg.pw, mode='fan_out',  nonlinearity='sigmoid')

    def forward(self, feats):
        level_ids = [i for i in range(self.levels)]
        return multi_apply(self.forward_single, feats, level_ids)

    def forward_single(self, x, level_id):
        cls_feat = x
        reg_feat = x
        for cls_conv, cls_bn_level in zip(self.cls_convs, self.cls_bn_levels):
            cls_feat = cls_conv(cls_feat)
            cls_feat = cls_bn_level[level_id](cls_feat)
        for reg_conv, reg_bn_level in zip(self.reg_convs, self.reg_bn_levels):
            reg_feat = reg_conv(reg_feat)
            reg_feat = reg_bn_level[level_id](reg_feat)
        cls_score = self.retina_cls(cls_feat)
        bbox_pred = self.retina_reg(reg_feat)
        return cls_score, bbox_pred

    def loss_single(self, cls_score, bbox_pred, labels, label_weights,
                    bbox_targets, bbox_weights, num_total_samples, cfg):
        # classification loss
        labels = labels.reshape(-1)
        label_weights = label_weights.reshape(-1)
        cls_score = cls_score.permute(0, 2, 3,
                                      1).reshape(-1, self.cls_out_channels)
        loss_cls = self.loss_cls(
            cls_score, labels, label_weights, avg_factor=num_total_samples)
        # regression loss
        bbox_targets = bbox_targets.reshape(-1, 4)
        bbox_weights = bbox_weights.reshape(-1, 4)
        bbox_pred = bbox_pred.permute(0, 2, 3, 1).reshape(-1, 4)
        loss_bbox = self.loss_bbox(
            bbox_pred,
            bbox_targets,
            bbox_weights,
            avg_factor=num_total_samples * 4)
        return loss_cls, loss_bbox

    @force_fp32(apply_to=('cls_scores', 'bbox_preds'))
    def loss(self,
             cls_scores,
             bbox_preds,
             gt_bboxes,
             gt_labels,
             img_metas,
             cfg,
             gt_bboxes_ignore=None):
        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]
        assert len(featmap_sizes) == len(self.anchor_generators)

        device = cls_scores[0].device

        anchor_list, valid_flag_list = self.get_anchors(
            featmap_sizes, img_metas, device=device)
        label_channels = self.cls_out_channels if self.use_sigmoid_cls else 1
        cls_reg_targets = anchor_target(
            anchor_list,
            valid_flag_list,
            gt_bboxes,
            img_metas,
            self.target_means,
            self.target_stds,
            cfg,
            gt_bboxes_ignore_list=gt_bboxes_ignore,
            gt_labels_list=gt_labels,
            label_channels=label_channels,
            sampling=self.sampling)
        if cls_reg_targets is None:
            return None
        (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list,
         num_total_pos, num_total_neg) = cls_reg_targets
        num_total_samples = (
            num_total_pos + num_total_neg if self.sampling else num_total_pos)
        losses_cls, losses_bbox = multi_apply(
            self.loss_single,
            cls_scores,
            bbox_preds,
            labels_list,
            label_weights_list,
            bbox_targets_list,
            bbox_weights_list,
            num_total_samples=num_total_samples,
            cfg=cfg)
        return dict(loss_cls=losses_cls, loss_bbox=losses_bbox)

